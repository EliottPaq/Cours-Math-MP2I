\chapter{Déterminants}

\minitoc

Dans ce chapitre, \(\K = \R\) ou \(\C\).
\section{Formes \(n-\)linéaires alternées}
Soit \(E\) un espace vectoriel sur \(\K\) de dimension finie non nulle \(n\).
\subsection{Définition}
\begin{defi}
    Une application \(f : E^n \to \K\) est dite forme \(n-\)linéaire alternée sur \(E\) si :
    \begin{enumerate}
        \item \(\forall j \in  \interventierii{1}{n} , \forall  (x_1, \dots  , x_{j-1}, x_{j+1}, \dots  , x_n) \in  E^{n-1}, (x \mapsto f \paren{x_1, \dots  , x_{j-1}, x, x_{j+1}, \dots  , x_n}) \in  \cal{L} (E, \K)\).
        \item \(f\) s’annule en tout \((x_1, \dots  , x_n) \in  E^n\) pour lequel il existe \((j, k) \in  \interventierii{1}{n}^2 \) avec \(j\neq k\) et \(x_j = x_k\).
    \end{enumerate}
\end{defi}

\subsection{Propriétés}
\begin{defprop}[Effet sur les familles liées]
    Si \(f\) est une forme \(n-\)linéaire  alternée sur \(E\) et \((x_1, \dots  , x_n)\) une famille de vecteurs de \(E\) liée alors \[f (x_1, \dots  , x_n) = 0\]
\end{defprop}
\begin{defprop}[Antisymétrie]
    Si \(f\) est une forme \(n-\)linéaire  alternée sur \(E\) alors \(f\) est antisymétrique, c’est-à-dire que :
    \[\forall  (x_1, \dots  , x_n) \in  E^n, \forall  (j, k) \in  \interventierii{1}{n}^2, j < k \imp f (x_1, \dots  , x_j , \dots  , x_k, \dots  , x_n) = -f (x_1, \dots  , x_k, \dots  , x_j , \dots  , xn) .\]
    \underline{Remarque}\\
    En notant \(\epsilon\)  la signature de \(\cal{S}_n\), l’antisymétrie de \(f\) se traduit par : pour tout \((x_1, \dots  , x_n) \in  E^n\) et pour toute transposition \(\theta\) de \(\interventierii{1}{n}\) , \(f\paren{x_{\theta (1)}, \dots  , x_{\theta (n)}} = \epsilon (\theta )f (x_1, \dots  , x_n) .\)
\end{defprop}
\begin{defprop}[Effet d’une permutation]
    Si \(f\) est une forme \(n-\)linéaire  alternée sur \(E\) alors :
    \[\forall \sigma  \in  \cal{S}_n, \forall  (x_1, \dots  , x_n) \in  E^n, f \paren{ x_{\sigma (1)}, \dots  , x_{\sigma (n)}} = \epsilon (\sigma )f \paren{x_1, \dots  , x_n} .\]
\end{defprop}

\section{Déterminant d’une famille de vecteurs dans une base}
    Soit \(E\) un espace vectoriel sur \(\K\) de dimension finie non nulle \(n\) muni d’une base \(\cal{B} = (e_1, \dots  , e_n)\) .
\subsection{Théorème}
\begin{theo}
    \begin{enumerate}
        \item Il existe une unique forme \(n-\)linéaire  alternée \(f\) sur \(E\) telle que \(f (\cal{B}) = f (e_1, \dots  , e_n) = 1\).
            Cette forme \(n-\)linéaire  alternée sur \(E\) est notée \(\det_{\cal{B}}\) et vérifie :
                \[\det_{\cal{B}}(\cal{B}) = 1.\]
        \item Toute forme \(n-\)linéaire  alternée \(g\) sur \(E\) est un multiple de \(\det_{\cal{B}}\) avec, plus précisément :
            \[g = g (\cal{B}) \det_{\cal{B}}.\]
    \end{enumerate}
\end{theo}
\begin{dem}
    On note \(\cal{A}_n\) l’ensemble des formes \(n-\)linéaires alternées sur \(E\) et on considère \(g \in \cal{A}_n\).\\~\\
    Soit \((x_1, \dots , x_n) \in E^n\).\\~\\
    Pour tout \(j \in \interventierii{1}{n}\) , on note \(x_j = \sum^n_{i=1} a_{i,j} e_i\) la décomposition de \(x_j\) dans la base \(\cal{B} = \paren{e_1, \dots , e_n}\) de \(E\).\\~\\
    Alors, par \(n-\)linéarité de \(g\), on a : \(g (x_1, \dots , x_n) = \sum_{(i_1,\dots,i_n)\in \interventierii{1}{n}^n} a_{i_1,1} \dots a_{i_n,n} \underbrace{g (e_{i_1} , \dots , e_{i_{n}} )}_{\substack{= 0\\ \text{ Si deux indices égaux}}}\)\\~\\
    Vu le caractère alterné de \(g\), on ne conserve dans la somme que les termes correspondant aux \(n-\)uplets \((i_1, \dots , i_n)\) d’éléments deux à deux distincts, c’est-à-dire les \(n-\)uplets \((\sigma (1), \dots , \sigma (n))\) avec \(\sigma\)  qui décrit le groupe symétrique \(\cal{S}_n\). Ainsi :
    \[g (x_1, \dots , x_n) = \sum_{\sigma \in\cal{S}_n} a_{\sigma (1),1} \dots a_{\sigma (n),n} g \paren{e_{\sigma (1)}, \dots , e_{\sigma (n)}}\]
    Comme \(g\) est forme \(n-\)linéaire alternée sur \(E\), on a : \(\forall \sigma  \in \cal{S}_n, g \paren{e_{\sigma (1)}, \dots , e_{\sigma (n)}  = \epsilon(\sigma )g(e_1, \dots , e_n)}\) où \(\epsilon(\sigma )\) est la signature de \(\sigma\) . Cela donne, après factorisation,
    \[\forall  (x_1, \dots , x_n) \in E^n, g (x_1, \dots , x_n) = \paren{\sum_{\sigma \in\cal{S}_n} \epsilon(\sigma ) \prod^n_{i=1} a_{\sigma (i),i}} \underbrace{g (e_1, \dots , e_n)}_{\in \K} \text{ donc } g = g (e_1, \dots , e_n)f \qquad \star\]

    en notant \(f : E^n \to \K\) la forme \(n-\)linéaire alternée définie par \(f : (x_1, \dots , x_n) \mapsto \paren{\sum_{\sigma \in\cal{S}_n}\epsilon(\sigma)\prod^n_{i=1} a_{\sigma (i),i}}\) avec \((a_{k,i})_{1\geq k\geq n}\) famille des coordonnées de \(x_k\).\\~\\
    Pour tout \(j \in \interventierii{1}{n}\) , on a \(e_j =\sum^n_{i=1}\delta_{i,j} e_i\) donc \(f (e_1, \dots , e_n) = \epsilon(\id{\cal{S}_n})\prod^n_{i=1} \delta_{\id{\N}(i),i}\) puis \(f (e_1, \dots , e_n) = 1\). La relation \((\star)\) assure de plus que \(f\) est la seule forme \(n-\)linéaire alternée sur \(E\) vérifiant cette égalité.\\~\\
    D’après \((\star)\), on a l’inclusion \(\cal{A}_n \Subset \Vect{F}\) puis \(\cal{A}_n = \Vect{f}\) (car tout \(g\) de \(\Vect{f}\) est forme \(n-\)linéaire alternée sur \(E\) puisque \(f\) l’est) avec \(\Vect{f}\) espace vectoriel de dimension \(1\) (car \(f\) n’est pas nulle puisqu’elle vaut \(1\) en \((e_1, \dots , e_n)\)).\\~\\
    \conclusion\\~\\
     L’ensemble des formes \(n-\)linéaires alternées sur \(E\) est un espace vectoriel de dimension \(1\) engendré par \(f : (x_1, \dots , x_n) \mapsto \paren{\sum_{\sigma \in\cal{S}_n} \epsilon(\sigma ) \prod^n_{i=1} a_{\sigma (i),i}}\), seule forme \(n-\)linéaire alternée sur \(E\) qui vérifie \(f (\cal{B}) = 1\).\\~\\
     En notant \(f = \det_{\cal{B}}\) (dit déterminant dans la base \(\cal{B}\) de \(E\)) , on a donc \(g = g (\cal{B}) \det_{\cal{B}} \) pour toute forme \(n-\)linéaire alternée \(g\) sur \(E\).
\end{dem}
\subsection{Déterminant d’une famille de vecteurs dans une base}
\begin{defi}
    Soit \((x_1, \dots  , x_n)\) une famille de \(n\) vecteurs de \(E\).\\~\\
    Le scalaire \(\det_{\cal{B}} (x_1, \dots  , x_n)\) est dit déterminant de la famille \((x_1, \dots  , x_n)\) dans la base \(\cal{B}\).
\end{defi}
\begin{defprop}[Expression du déterminant avec les coordonnées]
    Si \((x_1, \dots  , x_n)\) est une famille de \(n\) vecteurs de \(E\) et \(A = (a_{ij} )_{(i,j)\in \interventierii{1}{n}} = \Mat{\cal{B}}{x_1, \dots  , x_n}\) alors :
    \[\det_{\cal{B}}(x_1, \dots  , x_n) = \sum_{\sigma \in \cal{S}_n} \epsilon (\sigma ) \paren{\prod^{n}_{j=1} a_{\sigma (j)j}}\]

    \underline{Remarque}\\
    A l’issue de ce chapitre, on disposera de moyens plus pratiques que le recours à cette formule théorique pour calculer le déterminant d’une famille de vecteurs.
\end{defprop}

\begin{defprop}[Cas particuliers des dimension \(2\) et \(3\)]
    \begin{itemize}
        \item Si \(E\) est de dimension \(2\) et \((x, y)\) une famille de deux vecteurs de \(E\) de matrice \(\begin{pmatrix}
              x_{1} & y_{1} \\
              x_{2} & y_{2}
              \end{pmatrix}\) dans une base \(\cal{B}\) de \(E\) alors :
            \[\det_{\cal{B}}(x, y) = x_1y_2 - x_2y_1 \underset{\text{Not.}}{=}\begin{vmatrix}
              x_{1} & y_{1} \\
              x_{2} & y_{2}
              \end{vmatrix}\]
        \item Si \(E\) est de dimension \(3\) et \((x, y, z)\) une famille de trois vecteurs de \(E\) de matrice \(\begin{pmatrix}
              x_{1} & y_{1} & z_{1} \\
              x_{2} & y_{2} & z_{2} \\
              x_{3} & y_{3} & z_{3}
              \end{pmatrix}\) dans une base \(\cal{B}\) de \(E\) alors :
            \[\det_{\cal{B}}(x, y, z) = x_1 y_2 z_3 + x_2 y_3 z_1 + x_3 y_1 z_2 - x_3 y_2 z_1 - x_2 y_1 z_3 - x_1 y_3 z_2 \underset{\text{Not.}}{=} \begin{vmatrix}
              x_{1} & y_{1} & z_{1} \\
              x_{2} & y_{2} & z_{2} \\
              x_{3} & y_{3} & z_{3}
              \end{vmatrix}\]
    \end{itemize}
\end{defprop}
\subsection{Formule de changement de bases}
\begin{defprop}
    Si \(\cal{B}'\) est une autre base de \(E\) alors \(\det_{\cal{B}'} = \det_{\cal{B}'}(\cal{B}) \det_{\cal{B}}\) c’est-à-dire que :
    \[\forall  (x_1, \dots  , x_n) \in  E, det_{\cal{B}'} (x_1, \dots  , x_n) = \det_{\cal{B}'}(\cal{B}) \det_{\cal{B}} (x_1, \dots  , x_n) \]
\end{defprop}
\subsection{Caractérisation des bases par le déterminant}
\begin{defprop}
    Une famille \(\cal{B}' = (x_1, \dots  , x_n)\) de \(n\) vecteurs de \(E\) est une base de \(E\) si, et seulement si, \(\det_{\cal{B}}(\cal{B}')\neq 0\) avec dans ce cas,
    \[\det_{\cal{B}}(\cal{B}') = (\det_{\cal{B}'}(\cal{B}))^{-1} \]
    \underline{Remarque}\\
    En \(2^e\) année MPI, les notions d’orientation d’un \(\R-\)espace vectoriel de dimension finie et de bases directes ou indirectes seront définies en utilisant la notion de déterminant.
\end{defprop}
\section{Déterminant d’un endomorphisme ou d’une matrice carrée}
\subsection{Cas d’un endomorphisme en dimension finie}
    Soit \(E\) un espace vectoriel sur \(\K\) de dimension finie non nulle.
\begin{theo}
    Soit \(u\) un endomorphisme de \(E\).\\~\\
    Le scalaire \(\det_{\cal{B}} (u(\cal{B}))\), qui ne dépend pas de la base \(\cal{B}\) de \(\cal{E}\) considérée, est appelé déterminant de \(u\) et noté \(\det(u)\) :
    \[\det (u) = \det_{\cal{B}} (u(\cal{B})) \text{ avec }\cal{B} \text{ base quelconque de } E\]
    \underline{Exemples simples}\\
    \begin{itemize}
        \item En particulier, \( \det\paren{ 0_{\cal{L}(E)}} = 0\) et \(\det (\id{E} ) = 1\).
        \item Plus généralement,
        \begin{itemize}
            \item le déterminant d’une homothétie \(h_\alpha\)  de \(E\) de rapport \(\alpha\)  est : \(\det (h_{\alpha} ) = \alpha  \dim E\) .
            \item le déterminant d’une projection \(p\) de \(E\) autre que l’identité est : \(\det (p) = 0\).
            \item de déterminant d’une symétrie \(s\) de \(E\) est : \(\det (s) = (-1)^{\dim E-\dim F}\) où \( F = \ker(s - \id{E} )\)
        \end{itemize}
    \end{itemize}
\end{theo}
\begin{dem}
    On considère l’application \(g : E^n \to \K\) définie par
    \[\forall  (x_1, \dots , x_n) \in E^n, g (x_1, \dots , x_n) = \det_{\cal{B}'}(u(x_1), \dots , u(x_n))\]
    Montrons que \(g(\cal{B}') = \det_{\cal{B}} (u(\cal{B}))\) ce qui donnera le résultat souhaité.\\~\\
    Comme \(\det_{\cal{B}'}\) est une forme \(n-\)linéaire alternée sur \(E\) et que \(u\) est un endomorphisme de \(E\), on vérifie aisément que \(g\) est une forme \(n-\)linéaire alternée sur \(E\).\\~\\
    Par théorème, on a donc :
    \[g = g(\cal{B}) \det_{\cal{B}}\]
    En particulier :
    \[g(\cal{B}') = g(\cal{B}) \det_{\cal{B}} (\cal{B}') \]
    Or \(g(\cal{B}) = \det_{\cal{B}'} (u(\cal{B}))\) (par définition de \(g\)) et \(\det_{\cal{B}'} (u(\cal{B})) = \det_{\cal{B}'} (\cal{B}) \det_{\cal{B}}(u(\cal{B}))\) (par formule de changement de bases) donc :
    \[g(\cal{B}') = \det_{\cal{B}'} (\cal{B}) \det_{\cal{B}} (\cal{B}') \det_{\cal{B}} (u(\cal{B}))\]
    Comme \(\det_{\cal{B}}' (\cal{B}) \det_{\cal{B}} (\cal{B}') = \det_{\cal{B}'} (\cal{B}')\) (par formule de changement de bases) et \(\det_{\cal{B}'} (\cal{B}') = 1\) (par définition de \(\det_{\cal{B}'}\) ), on en déduit que :
    \[g(\cal{B}') = \det_{\cal{B}} (u(\cal{B}))\]
    autrement dit
    \[\det_{\cal{B}'} (u(\cal{B}')) = \det_{\cal{B}} (u(\cal{B})) \]
\end{dem}
\begin{prop}
    \begin{enumerate}
        \item \(\forall \lambda \in  \K, \forall u \in  \cal{L}(E), \det (\lambda u) = \lambda ^{\dim E} det (u)\)
        \item \(\forall (u, v) \in  (\cal{L}(E))^2 , \det (v \circ u) = \det (v) \det (u) = \det (u) \det (v)\)
        \item \(\forall q \in \N, \forall u \in  \cal{L}(E) , \det (u^q) = (\det u)^q \)
    \end{enumerate}
\end{prop}

\begin{defprop}[Caractérisation des automorphismes avec le déterminant]
    Un endomorphisme \(u\) de \(E\) est bijectif si, et seulement si, \(\det (u)\neq 0\) avec dans ce cas,
    \[det (u^{-1}) = (\det (u))^{-1}\]
\end{defprop}
\begin{defprop}[Morphisme de groupes de \(\cal{GL}(E)\) vers \(\Ks\)]
    L’application \(u \in  \cal{GL}(E) \mapsto \det (u)\) est un morphisme de groupes de \(\paren{\cal{GL}(E), \circ}\) vers \(\paren{\Ks, \times}\) .
\end{defprop}
\subsection{Déterminant d’une matrice carrée}
    Soit \(n \in  \Ns\).
\begin{defi}
    On appelle déterminant d’une matrice \(A \in  \M{n}\), et on note \(\det (A)\), le déterminant de l’endomorphisme de \(\K^n\) canoniquement associé à \(A\).\\~\\
    \underline{Exemples simples}\\
    En particulier, \(\det \paren{0_{\M{n}}} = 0\) et \(\det (I_n) = 1\).  
\end{defi}
\begin{defprop}[Caractère \(n-\)linéaire  alterné du déterminant par rapport aux colonnes]
    Si \(C_1, \dots  , C_n\) sont les colonnes de \(A \in  \M{n}\) et \(B_{can}\) est la base canonique de \(\K^n\) alors
    \[\det(A) = \det_{B_{can}} (C_1, \dots  , C_n)\]
    donc le déterminant d’une matrice carrée est \(n-\)linéaire  alterné par rapport aux colonnes.
\end{defprop}

\begin{prop}
    Soit \(A\) et \(\cal{B}\) deux matrices de \(\M{n}, \lambda \in  \K\) et \(q \in \N\).
    \begin{enumerate}
        \item \(\det(A) = -det(A')\) où \(A'\) est la matrice obtenue en échangeant deux colonnes de \(A\).
        \item Si deux colonnes de \(A\) sont égales (resp. si les colonnes de \(A\) sont liées) alors \(\det(A) = 0\).
        \item \(\det(\lambda A) = \lambda^n \det(A)\)
        \item \(\det(AB) = \det(A) \det(\cal{B}) = \det(\cal{B}) \det(A)\).
        \item \(\det(A^q) = (\det(A))^q\).
    \end{enumerate}
\end{prop}

\begin{defprop}[Caractérisation des matrices inversibles avec le déterminant]
    Une matrice \( A \in  \M{n}\) est inversible si, et seulement si, \(\det(A) \neq 0\) avec, dans ce cas, \[\det(A^{-1}) = (\det(A))^{-1}\]
\end{defprop}

\begin{defprop}[Morphisme de groupes de \(\cal{GL}_n(\K)\) vers \(\Ks\)]
    L’application \(A \in  \cal{GL}_n(\K) \mapsto \det(A)\) est un morphisme de groupes de \((\cal{GL}_n(\K), \times)\) vers \((\Ks, \times)\) .
\end{defprop}

\begin{defprop}[Déterminant de matrices semblables]
    Si \(A\) et \(\cal{B}\) sont des matrices semblables alors \(\det(A) = \det(\cal{B})\).
\end{defprop}
\begin{dem}
    Montrons que la réciproque est fausse \\~\\
    Soit \(A = \begin{pmatrix}
        1 & 0\\
        0 & 1
    \end{pmatrix}\) et \(B = \begin{pmatrix}
        2 & 0\\
        0 & \frac{1}{2}
    \end{pmatrix}\)\\~\\
    on a bien \(\det(A) = \det(B)\) \\
    Or \(A\) n'est pas semblable à B car sinon on aurait \(B = P^{-1} A P = P^{-1} P = I_n\) ce qui est faux
\end{dem}
\begin{defprop}[Expression du déterminant à l’aide des coefficients de la matrice]
    Si \(A = (a_{ij} )_{(i,j)\in \interventierii{1}{n}^2} \in \M{n}\) alors
    \[\det (A) = \sum_{\sigma \in \cal{S}_n} \epsilon (\sigma ) \paren{\prod^n_{j=1} a_{\sigma(j)j}}\]
\end{defprop}

\begin{defprop}[Déterminant et transposition]
    Les déterminants d’une matrice carrée et de sa transposée sont égaux :
    \[\forall A \in  \M{n} , \det(\trans{A}) = \det(A)\]
    \underline{Remarque}\\
    Les propriétés vues sur le déterminant d’une matrice carrée relatives à ses colonnes s’étendent aux lignes.
\end{defprop}
\begin{dem}
    Montrons que \(\forall A \in  \M{n} , \det(\trans{A}) = \det(A)\)
    \begin{align*}
        \det(A) &= \sum_{\sigma \in \cal{S}_n} \epsilon (\sigma ) \paren{\prod^{n}_{i=1} a_{\sigma (i)i}}\\
        &= \sum_{\sigma \in \cal{S}_n} \epsilon (\sigma ) \paren{\prod^{n}_{j=1} a_{j\sigma^{-1}(j)}}\qquad \text{ Changement d'indices }i = \sigma^{-1}(j) \text{ possible car } \sigma \text{ est une bijection} \\
        & = \sum_{\sigma' \in \cal{S}_n} \epsilon (\sigma' ) \paren{\prod^{n}_{j=1} a_{j\sigma'(j)}} \qquad \sigma ' = \sigma^{-1} \text{ possible car } \epsilon(\sigma) = \epsilon(\sigma^{-1})\\
        &=\sum_{\sigma \in \cal{S}_n} \epsilon (\sigma ) \paren{\prod^{n}_{j=1} a'_{\sigma'(j)j}} \qquad a'_{1,t} = a_{t,1}\\
        &=\det(\trans{A})
    \end{align*}
\end{dem}
\subsection{Calcul de déterminants en pratique}
    On note \(C_1, \dots  , C_n\) les colonnes (resp. \(L_1, \dots  , L_n\) les lignes) de \(A = (a_{ij} )_{1\leq i,j\leq n} \in  \M{n}\).
\begin{defprop}[Effet des opérations élémentaires sur un déterminant]
    \begin{enumerate}
        \item L’opération élémentaire \(C_r \leftarrow \lambda C_r\) multiplie le déterminant par \(\lambda\).
        \item L’opération élémentaire \(C_r \leftrightarrow C_s\) avec \(r\neq s\) multiplie le déterminant par \(-1\).
        \item L’opération élémentaire \(C_r \leftarrow C_r + \lambda C_s\) avec \(r\neq s\) ne modifie pas le déterminant.
    \end{enumerate}
    \underline{Remarque}\\
    On obtient le même résultat avec les opérations élémentaires sur les lignes.
\end{defprop}

\begin{defprop}[Développement suivant une colonne (ou une ligne)]
    Pour \((i, j) \in  \interventierii{1}{n}^2\), on appelle cofacteur du coefficient \(a_{ij}\) de \(A\) le scalaire noté \(A_{ij}\) défini par :
    \[A_{ij} = (-1)^{i+j} M_{ij}\]
    où \(M_{ij}\) est le déterminant de la matrice extraite de \(A\) obtenue en supprimant \(L_i\) et \(C_j\) .
    \begin{enumerate}
        \item Formule de calcul du déterminant par développement suivant la \(j^e\) colonne.
        \[\det(A) = a_{1j} A_{1j} + a_{2j} A_{2j} + \dots + a_{nj} A_{nj} = \sum^n_{i=1} a_{ij} A_{ij}\] 
        \item Formule de calcul du déterminant par développement suivant la \(i^e\) ligne.
        \[\det(A) = a_{i1}A_{i1} + a_{i2}A_{i2} + \dots + a_{in}A_{in} = \sum^n_{j=1} a_{ij} A_{ij} \]
    \end{enumerate}
    \underline{Remarque :}\\
    Sauf cas particulier, avant d’utiliser ces formules de développement, on effectuera des opérations élémentaires pour obtenir un maximum de zéros ou un facteur commun sur une ligne/colonne.
\end{defprop}
\begin{dem}
    On note \(C_1, \dots , C_n\) les colonnes (resp \(L_1, \dots , L_n\) les lignes) de \(A\) et \(\cal{B} = (e_1, \dots , e_n)\) la base usuelle de \(\K^n\).\\~\\
    Soit \(j \in \interventierii{1}{n}\).\\
    Par définition du déterminant de \(A\),on a :
    \[\det (A) = \det_{\cal{B}} (C_1, \dots , C_j , \dots , C_n) = \det_{\cal{B}}\paren{ C_1, \dots , \sum^n_{i=1} a_{ij} e_i, \dots , C_n}\]
    Par linéarité du déterminant d’une matrice par rapport à chacune des ses colonnes, on trouve :
    \[\det (A) = \sum^n_{i=1} a_{ij} \det_{\cal{B}} \paren{C_1, \dots , e_i, \dots , C_n} \qquad(\star)\]
    On note
    \[A_{ij} = \det_{\cal{B}} \paren{C_1, \dots C_{j-1}, e_i, C_{j+1}, \dots , C_n} \]
    Comme l’échange de deux colonnes multiplie le déterminant par \(-1\), en intervertissant successivement les colonnes \(j\) et \(j + 1\), puis \(j + 1\) et \(j + 2\) jusqu’aux colonnes \(n - 1\) et \(n\) ( ce qui fait \(n - j\) interversions), on trouve :
    \[A_{ij} = (-1)^{n-j} \det_{\cal{B}} \paren{C_1, \dots C_{j-1}, C_{j+1}, C_{j+2}, \dots , C_n, e_j } \].
    On procède de même avec les lignes en intervertissant successivement les lignes \(i\) et \(i + 1\), puis \(i + 1\) et \(i + 2\) jusqu’aux lignes \(n - 1\) et \(n\) (ce qui fait \(n - i\) interversions) et on trouve :
    \[A_{ij} = (-1)^{n-j} (-1)^{n-i} \det 
    \begin{pmatrix}
    a_{1,1}   & \dots & a_{1,j-1}   & a_{1,j+1}   & \dots & a_{1,n}  & 0      \\
    \vdots    &       & \vdots      & \vdots      &       & \vdots   & \vdots \\
    a_{i-1,1} & \dots & a_{i-1,j-1} & a_{i-1,j+1} & \dots & a_{i-1,n}& 0      \\
    a_{i+1,1} & \dots & a_{i+1,j-1} & a_{i+1,j+1} & \dots & a_{i+1,n}& 0      \\
    \vdots    &       & \vdots      & \vdots      &       & \vdots   & \vdots \\
    a_{n,1}   & \dots & a_{n,j-1}   & a_{n,j+1}   & \dots & a_{n,n}  & 0      \\
    a_{i,1}   & \dots & a_{i,j-1}   & a_{i,j+1}   & \dots & a_{i,n}  & 1 
    \end{pmatrix}
    \]
    Ainsi, \(A_{ij} = (-1)^{i+j} \det B_{ij}  \qquad (\star\star)\) où \(B_{ij}\) est la matrice définie par blocs
    \[B_{ij} =\begin{pmatrix}
        C_{ij} & 0_{n-1,1} \\
        D_{ij} & 1         
    \end{pmatrix}\]
    avec
    \begin{itemize}
        \item \(C_{ij}\) matrice extraite de \(A\) en otant \(L_i\) et \(C_j\)
        \item \(D_{ij}\) ligne extraite de \(L_i\) en otant sa colonne \(j\).
    \end{itemize}
    Montrons que \(\det B_{ij} = \det C_{ij}\)\\
    (cas particulier d’un résultat général qui sera montré en MPI : le déterminant d’une matrice triangulaire par blocs est égal au produit des déterminants de ses blocs diagonaux)\\~\\
    Pour simplifier, on note \(B\) (resp. \(C\)) et pas \(B_{ij}\) (resp. \(C_{ij}\) ) avec \(\cal{B} = (b_{i,j} )_{(i,j) \in\interventierii{1}{n}^2}\) et \(C = (c_{i,j})_{(i,j)\in \interventierii{1}{n-1}^2}\).\\~\\
    On sait que
    \[\det B = \sum_{\sigma \in\cal{S}_n} \epsilon(\sigma )b_{\sigma (1),1}b_{\sigma (2),2} \dots b_{\sigma (n),n}\]
    avec, par définition de \(B, b_{\sigma (n),n} = 
    \begin{cases}
        0 \text{ si }\sigma (n)\neq n \\
        1 \text{ si }\sigma (n) = n\\
    \end{cases}
    \)\\
    Ainsi :
    \[\det B = \sum_{\sigma \in T_n} \epsilon(\sigma )b_{\sigma (1),1}b_{\sigma (2),2} \dots b_{\sigma (n-1),n-1}\]
    avec \(T_n = \accol{\sigma  \in \cal{S}_n \tq \sigma (n) = n}\) (\ie l’ensemble des permutations de \(\interventierii{1}{n}\) qui laissent \(n\) invariant).\\~\\
    \(\psi : T_n \to \cal{S}_{n-1}\) définie par \(\forall \sigma  \in T_n, \psi(\sigma ) = \sigma '\) avec \(\sigma  \in \cal{S}_{n-1}\) tel que \(\forall i \in \interventierii{1}{n-1} , \sigma '(i) = \sigma (i)\)
    \begin{itemize}
        \item est bijective (de bijection réciproque \(\psi : \cal{S}_{n-1} \to T_n\) définie par \(\forall \sigma ' \in \cal{S}_{n-1}, \psi(\sigma ') = \sigma\)  avec \(\sigma  \in \cal{S}_n\) tel que \(\forall i \in \interventierii{1}{n-1} , \sigma (i) = \sigma '(i)\) et \(\sigma (n) = n\))
        \item  conserve la signature des permutations \ie \(\forall \sigma  \in T_n, \epsilon (\psi(\sigma )) = \epsilon(\sigma )\) car, comme \(\sigma\)  et \(\sigma ' = \psi(\sigma )\)coïncident sur \(\interventierii{1}{n-1}\) et que \(n\) est invariant par \(\sigma\) , les permutations \(\sigma\)  et \(\psi(\sigma )\) peuvent se décomposer en le même produit de transpositions donc ont même signature.
    \end{itemize}
    On en déduit, avec le changement d’indice bijectif \(\sigma ' = \psi(\sigma )\), que 
    \[\det B = \sum_{\sigma '\in\cal{S}_{n-1}}\epsilon(\sigma ')b_{\sigma '(1),1}b_{\sigma '(2),2} \dots b_{\sigma '(n-1),n-1}\]
    Vu le lien entre les coefficients de \(C\) et ceux de \(\cal{B}\), on en déduit que \(\det \cal{B} = \sum_{\sigma '\in\cal{S}_{n-1}} \epsilon(\sigma ')c_{\sigma '(1),1}c_{\sigma '(2),2} \dots c_{\sigma '(n-1),n-1}\) ce qui donne, par formule théorique du déterminant de \(C\) :
    \[\det B = \det C\]
    \conclusion avec les résultats \((\star)\) et \((\star\star)\) trouvés, on en déduit que :
    \[\det(A) = a_{1j} A_{1j} + \dots + a_{nj} A_{nj} = \sum^n_{i=1} a_{ij} A_{ij} \]
    où \(A_{ij} = (-1)^{i+j} M_{ij}\) où \(M_{ij} = \det (C_{ij} )\) avec \(C_{ij}\) , matrice extraite de \(A\) en otant \(L_i\) et \(C_j\).
\end{dem}

\begin{defprop}[Déterminant de matrices particulières]
    \begin{enumerate}
        \item Le déterminant d’une matrice triangulaire est le produit de ses éléments diagonaux.
        \item Déterminant de la matrice de Vandermonde pour \((x_1, \dots  , x_n) \in  \K^n \).
        \[\det\begin{pmatrix}
            1 & x_1 & \dots & x^{n-1}_1\\
            1 & x_2 & \dots & x^{n-1}_2\\
            \vdots & \vdots &&\vdots \\
            1 & x_n &\dots&  x^{n-1}_n 
            \end{pmatrix} = \prod_{1\leq s<t\leq n}(x_t - x_s)
        \]
    \end{enumerate}
\end{defprop}
\begin{dem}
    Soit \((x_1, \dots  , x_n) \in  \K^n\)\\~\\
    On note \(V(x_1, \dots,x_n) =  \begin{vmatrix}
            1 & x_1 & \dots & x^{n-1}_1\\
            1 & x_2 & \dots & x^{n-1}_2\\
            \vdots & \vdots &&\vdots \\
            1 & x_n &\dots & x^{n-1}_n 
            \end{vmatrix} \)\\~\\
    Montrons que \(\forall n \in \N, n \geq 2,V(x_1, \dots,x_n)  =\prod_{1\leq s<t\leq n}(x_t - x_s) \) par récurrence forte
    \begin{itemize}
        \item Soit \(n = 2,\begin{vmatrix}
            1 & x_1 \\
            1 & x_2
        \end{vmatrix} = x_2-x_1\)
        \item supposons la propriété vrai pour tout \(n\) inférieur à \(n+1\), Montrons alors cette propriété au rang \(n+1\)\\~\\
        Alors \(V(x_1, \dots,x_{n+1}) =  \begin{vmatrix}
            1 & x_1 & \dots & x^{n}_1\\
            1 & x_2 & \dots & x^{n}_2\\
            \vdots & \vdots &&\vdots \\
            1 & x_n &\dots & x^{n}_n 
            \end{vmatrix} \)\\~\\
            On cherche \(P = x^n + a_{n-1}x^{n-1} + \dots a_1 x + a_0\) tel que \(\forall i \in \interventierii{1}{n} P(x_i) = 0\)\\
            On remarque que \(P = \prod_{k=1}^n (x-x_k)\) convient \\
            Ainsi en faisant \(C_{n+1} \leftarrow C_{n+1} + a_{n-1}C_n + \dots a_1 C_2 + a_0 C_1\) sur les colonnes, alors d'après les propriété de \(P\) sur les \(x_i\) \\
            On obtient \begin{align*}
            V(x_1, \dots,x_{n+1}) &=  \begin{vmatrix}
            1 & x_1 & \dots & 0\\
            1 & x_2 & \dots & 0\\
            \vdots & \vdots &&\vdots \\
            1 & x_n &\dots & P
            \end{vmatrix}\\
            &\underset{\text{dévellopement par rapport à }C_{n+1}}{=} P(x_{n+1}) \prod_{1\leq s<t\leq n}(x_t - x_s) \\
            &= \prod_{1\leq s<t\leq n+1}(x_t - x_s)
        \end{align*}
    \end{itemize}
\end{dem}

\subsection{Comatrice}
\begin{defi}
    Soit \(A = (a_{ij} )_{(i,j)\in \interventierii{1}{n}^2} \in  \M{n}\) .\\~\\
    On appelle comatrice de \(A\), et on note \(\Com{A}\), la matrice de \(\M{n}\) définie par 
    \[\Com{A} = (A_{ij} )_{(i,j) \in \interventierii{1}{n}^2}\]
    où \(A_{ij}\) est le cofacteur de l’élément \(a_{ij}\) de \(A\).\\
    \underline{Remarque}\\
    Autrement dit, la comatrice de \(A\) est la matrice des cofacteurs de \(A\).
\end{defi}

\begin{defprop}[Relation liant \(A\) et sa comatrice]
    Pour toute matrice \(A\) de \(\M{n}\), on a :
    \[A \trans{\Com{A}} = \trans{\Com{A}} A = \det (A)I_n.\]
\end{defprop}
\begin{defprop}[Expression de l’inverse d’une matrice]
    Si \(A\) est une matrice inversible de \(\M{n}\) alors
    \[A^{-1} = \frac{1}{\det (A)} \trans{\Com{A}} \]
    \underline{Remarque}\\
    Cette formule est à bannir pour des calculs pratiques lorsque \(n \geq 3\) en raison de la lourdeur des calculs qu’elle implique.
\end{defprop}